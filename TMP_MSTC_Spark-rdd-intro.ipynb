{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to <font color=blue>pySpark</font> https://spark.apache.org/\n",
    "\n",
    "![Image of Sklearn](https://spark.apache.org/images/spark-logo-trademark.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resilient Distributed Dataset** or **RDD**. An RDD is a distributed collection of elements.\n",
    "\n",
    "* Creating new RDDs, **transforming** existing RDDs, or calling **actions** on RDDs to compute a result.\n",
    "\n",
    "* Spark automatically distributes the data contained in RDDs across your cluster and parallelizes the operations you perform on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIRST check SparkContext is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET SparkContext configuration will be \"local\" see spark.master\n",
    "\n",
    "* ### <font color=red>local[*]</font> means Spark locally with as many worker threads as logical cores on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'>--------------------------To Do:---------------------------</font>\n",
    "\n",
    "* ### how many cores do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import .....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a RDD from a file  : we will use <font color=orange>churn-bigml-80.csv</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common way of creating an RDD is to load it from a file.\n",
    "\n",
    "(Spark's `textFile` can handle compressed files directly, as gz).    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'>--------------------------To Do:---------------------------</font>\n",
    "\n",
    "* ### load churn-bigml-80.csv as a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_file = ....\n",
    "raw_data = sc.textFile(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our data file loaded into the `raw_data` RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without getting into Spark *transformations* and *actions*, the most basic thing we can do to check that we got our RDD contents right is to `count()` the number of lines loaded from the file into the RDD.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the first few entries in our data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'>--------------------------To Do:---------------------------</font>\n",
    "\n",
    "* ### take first line: first_line\n",
    "* ### create a `list` with features names: features\n",
    "* ### print..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_line=\n",
    "\n",
    "# create a list with features\n",
    "features = \n",
    "print(\"number of elements =\",.....)\n",
    "print(\"Features: \\n\",.....)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check: pySpark \"structures\" RDD vs python ... lists, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following notebooks, we will use this raw data to learn about the different Spark transformations and actions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and RDD using `parallelize`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of creating an RDD is to parallelize an already existing list.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <font color='magenta'>--------------------------To Do:---------------------------</font>\n",
    "\n",
    "* ### Create  range 0,1,2,.... 1000\n",
    "\n",
    "... discuss range and list(range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = \n",
    "\n",
    "data = sc.parallelize(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'>--------------------------To Do:---------------------------</font>\n",
    "\n",
    "* As we did before, we can count the number of elements in the RDD.\n",
    "* and show the first five elements..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can access the first few elements on our RDD.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=brown>Let's see now TRANSFORMATIONS and ACTIONS</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The `filter` transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transformation can be applied to RDDs in order to keep just elements that satisfy a certain condition. More concretely, a function is evaluated on every element in the original RDD. The new resulting RDD will contain just those elements that make the function return `True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, imagine we want to count how many lines contain the word `True` in our dataset.\n",
    "\n",
    "We will filter our `raw_data` RDD as described below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ... We are going to use a `lambda` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'>--------------------------To Do:---------------------------</font>\n",
    "\n",
    "* ### Define a function `contrainsTrue` that receives a list and returns True or False depending on if the string True is present in the list\n",
    "\n",
    "* ### Use a lambda function for the same task..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ...\n",
    "\n",
    "....\n",
    "\n",
    "containsTrue(['1','True'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now use a lambda function for the same task\n",
    "\n",
    "f = ....\n",
    "\n",
    "\n",
    "f(['1','True'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'>--------------------------To Do:---------------------------</font>\n",
    "\n",
    "* ### FILTER RDD raw_data depending on if the string True is present in its rows using `lambda` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "True_raw_data = raw_data.filter(........)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... `count` is the ACTION\n",
    "\n",
    "Now we can count how many elements we have in the new RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "t0 = time()\n",
    "True_count = True_raw_data.count()\n",
    "tt = time() - t0\n",
    "print(\"There are {} lines containing 'True' word\".format(True_count))\n",
    "print(\"Count completed in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have measured the elapsed time for counting the elements in the RDD. We have done this because we wanted to point out that actual (distributed) computations in Spark take place when we execute *actions* and not *transformations*. In this case `count` is the action we execute on the RDD. We can apply as many transformations as we want on a our RDD and no computation will take place until we call the first action that, in this case takes a few seconds to complete.\n",
    "<font color='green' size=4>And this is the \"Lazy Evaluation\"!!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** RDD Operations and Lazy Evaluation ** ###\n",
    "\n",
    "### <font color='green'>Lazy Evaluation</font> ###\n",
    "##### Lazy evaluation reduces the number of passes to take over data by grouping operations together. #####\n",
    "##### - In MapReduce systems like Hadoop, developers often have to spend a lot time considering how to group together operations to minimize the number of MapReduce passes. #####\n",
    "###### - In Spark, there is no substantial benefit to writing a single complex map instead of chaining together many simple operations. Thus, users are free to organize their program into smaller, more manageable operations. #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In this simple example the name of the Churn file is wrong (800 instead of 80)\n",
    "#\n",
    "#  select lines in log.txt with \"error\" messages\n",
    "\n",
    "data_file_err = '/resources/data/MSTC/churn-bigml-800.csv'\n",
    "raw_data_err = sc.textFile(data_file_err)\n",
    "\n",
    "True_raw_data_err = raw_data_err.filter(lambda x: 'True' in x)\n",
    "\n",
    "# but you will get NO error when executing this cell!  as only a transformation (i.e. filter()) is applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Error shows up NOW when performing actions (in this case .count()) (beacause Lazy eval)\n",
    "\n",
    "True_count = True_raw_data_err.count()\n",
    "\n",
    "# see error traces below:\n",
    "# Input path does not exist: file: .... nb2-rdd-basics/kddcup.data_1000_percent.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `map` transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the `map` transformation in Spark, we can apply a function to every element in our RDD. Python's lambdas are specially expressive for this particular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we want to read our data file as a CSV formatted one. We can do this by applying a lambda function to each element in the RDD as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'>--------------------------To Do:---------------------------</font>\n",
    "\n",
    "* ### MAP RDD raw_data `splitting` each row by comas using `lambda` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_data = raw_data......\n",
    "\n",
    "# see or print some csv_data rows...\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'>--------------------------To Do:---------------------------</font>\n",
    "\n",
    "* ### Now insert MAP RDD raw_data `splitting` to print some rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from pprint import pprint\n",
    "csv_data = ....\n",
    "t0 = time()\n",
    "head_rows = csv_data.take(5)\n",
    "tt = time() - t0\n",
    "print(\"Parse completed in {} seconds\".format(round(tt,3)))\n",
    "pprint(head_rows[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `collect` action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have used the actions `count` and `take`. Another basic action we need to learn is `collect`. Basically it will get all the elements in the RDD into memory for us to work with them. For this reason it has to be used with care, specially when working with large RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'>--------------------------To Do:---------------------------</font>\n",
    "\n",
    "* ### Calleect all RDD raw_data into all_raw_data \n",
    "<br>\n",
    "\n",
    "\n",
    "<font color='red' size=4>NOTE: (This may be dangerous when large datasets!!!) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "all_raw_data = ......\n",
    "tt = time() - t0\n",
    "print(\"Data collected in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That COULD took longer as any other action we used before, of course. Every Spark worker node that has a fragment of the RDD has to be coordinated in order to retrieve its part, and then *reduce* everything together.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As a last example: try do some TASK combining all the previous steps....\n",
    "\n",
    "To be defined by 4th december 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get data from file\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
